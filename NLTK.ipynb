{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0e6993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Tekenization/Tokenizer\\n2. Stemming and Lammatization\\n3. Stopwords Removal/ Punctuation Mark Removal\\n4. Vectorization (Count Vectorization and TFIDF Vectorization)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing of text or it is also called as text mining\n",
    "'''\n",
    "1. Tekenization/Tokenizer\n",
    "2. Stemming and Lammatization\n",
    "3. Stopwords Removal/ Punctuation Mark Removal\n",
    "4. Vectorization (Count Vectorization and TFIDF Vectorization)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff5e1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In NLP, Dataset will be classification type if supervised learning algorithm where we have to predict email's message is\n",
    "# a spam or not? or Predict review of a movie is positive or negative? or Predict news is fake or not?\n",
    "# In NLP, Input of dataset is pure text format or paragraph format and Output or Target is Categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8419f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install NLTK(Natural Language Toolkit) package for NLP\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df602e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09591a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some library which hold in nltk package\n",
    "# only first time download\n",
    "# nltk.download('punkt')    # punkt for punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e80816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords') # stopwords means he, she, it, is, are, was, were etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb739bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b22203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing:\n",
    "# Thre are 2 component of NLP:\n",
    "# 1. NLU: Natural Language Understanding\n",
    "# 2. NLG: Natural Language Generation\n",
    "# Step-1: Tokenization is the first steps of NLU : natural language\n",
    "# Understanding \n",
    "# Tokenization is the process of dividing the whole text into tokens and \n",
    "# whole documents /paragraph into sentence.\n",
    "# It is mainly of two types:\n",
    "\n",
    "# Word Tokenizer (separated by words)\n",
    "# Sentence Tokenizer (separated by sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61067bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizer: use inbuilt class word_tokenize\n",
    "# word_tokennize is defined in nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941ce592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram is a good boy.\n"
     ]
    }
   ],
   "source": [
    "# for example\n",
    "txt=\"Ram is a good boy.\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6430a221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ram', 'is', 'a', 'good', 'boy.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803d37e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'is', 'a', 'good', 'boy', '.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize() # cutting the wordn into small pieces and hold answer in list\n",
    "w=word_tokenize(txt)\n",
    "print(w)\n",
    "print(type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8c6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenizer, use inbuilt class sent_tokenize which is defined in nltk.tokenize\n",
    "para='''\n",
    "Understanding context is also an issue – something that requires semantic analysis for machine learning to get a handle on \n",
    "it. Natural language understanding (NLU) is a sub-branch of NLP and deals with these nuances via machine reading \n",
    "comprehension rather than simply understanding literal meanings. The aim of NLP and NLU is to help computers understand\n",
    "human language well enough that they can converse in a natural way.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1182c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "para=para.lower() # converts all words in lower case to check with the stopwords in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d0f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph Tokenizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7497a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nunderstanding context is also an issue – something that requires semantic analysis for machine learning to get a handle on \\nit.', 'natural language understanding (nlu) is a sub-branch of nlp and deals with these nuances via machine reading \\ncomprehension rather than simply understanding literal meanings.', 'the aim of nlp and nlu is to help computers understand\\nhuman language well enough that they can converse in a natural way.']\n"
     ]
    }
   ],
   "source": [
    "s=sent_tokenize(para)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bdacc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understanding', 'context', 'is', 'also', 'an', 'issue', '–', 'something', 'that', 'requires', 'semantic', 'analysis', 'for', 'machine', 'learning', 'to', 'get', 'a', 'handle', 'on', 'it', '.', 'natural', 'language', 'understanding', '(', 'nlu', ')', 'is', 'a', 'sub-branch', 'of', 'nlp', 'and', 'deals', 'with', 'these', 'nuances', 'via', 'machine', 'reading', 'comprehension', 'rather', 'than', 'simply', 'understanding', 'literal', 'meanings', '.', 'the', 'aim', 'of', 'nlp', 'and', 'nlu', 'is', 'to', 'help', 'computers', 'understand', 'human', 'language', 'well', 'enough', 'that', 'they', 'can', 'converse', 'in', 'a', 'natural', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "w=word_tokenize(para)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b25aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2 of Text Preprocessing: To remove Stopwords\n",
    "# stopwords means he/she/it/was/were etc.\n",
    "\n",
    "# To show list of all stopwords, all stopwords are stored in inbuilt library nltk.stopwords\n",
    "# call inbuilt class stopwords which is defined in nltk.corpus\n",
    "# corpus means corpora: it is a knowledge based database which holds all the stopwords in this database.\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c62e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Create object of stopwords class\n",
    "stop=stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2361dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understanding', 'context', 'also', 'issue', '–', 'something', 'requires', 'semantic', 'analysis', 'machine', 'learning', 'get', 'handle', '.', 'natural', 'language', 'understanding', '(', 'nlu', ')', 'sub-branch', 'nlp', 'deals', 'nuances', 'via', 'machine', 'reading', 'comprehension', 'rather', 'simply', 'understanding', 'literal', 'meanings', '.', 'aim', 'nlp', 'nlu', 'help', 'computers', 'understand', 'human', 'language', 'well', 'enough', 'converse', 'natural', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "# To remove stopwords from given list word\n",
    "new_list=[word for word in w if word not in stop]\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d237d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step 3: To remove punctuation mark\n",
    "# inbuilt class punctuation which is defined in string package\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5e6a920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# To show the list of punctuation\n",
    "# Create object of punctuation class\n",
    "punc=list(punctuation)\n",
    "print(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d5d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understanding', 'context', 'also', 'issue', '–', 'something', 'requires', 'semantic', 'analysis', 'machine', 'learning', 'get', 'handle', 'natural', 'language', 'understanding', 'nlu', 'sub-branch', 'nlp', 'deals', 'nuances', 'via', 'machine', 'reading', 'comprehension', 'rather', 'simply', 'understanding', 'literal', 'meanings', 'aim', 'nlp', 'nlu', 'help', 'computers', 'understand', 'human', 'language', 'well', 'enough', 'converse', 'natural', 'way']\n"
     ]
    }
   ],
   "source": [
    "new_list=[word for word in new_list if word not in punc]\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7bf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming: 4th step of text preprocessing\n",
    "# Step 4: Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fa0cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object of PorterStemmer class\n",
    "porter=PorterStemmer() # Porter is a user defined object of PorterStemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "266c5dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'replac'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem() is inbuilt method of PorterStemmer class\n",
    "porter.stem('replacement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3157e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('walked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6483276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studi'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21876fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inning'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('innings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "513acca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crouch'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('crouching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23b356e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sturdi'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('sturdy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003ea96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daili'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfa63cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization: nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54c7c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object of WordNetLemmatizer class\n",
    "lemma=WordNetLemmatizer() # lemma is a user defined object of WordNetLemmatizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15028171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('omw-1.4') # if it not working for lemmatization directly then download this version for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5f5d6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studying'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize() inbuilt method of WordNetLemmatizer class\n",
    "lemma.lemmatize('studying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7395afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daily'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de2aa554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sturdy'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('sturdy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4835fe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('studying', pos=wordnet.VERB) # bydefault pos=wordnet.NOUN, pos means part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1420a136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('studied', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b97e9cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crouch'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('crouching', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46773526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tumble'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('tumbling', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "955133f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cross'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('crossing', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b84dbfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thieve'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('thieves', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1614504e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thieve'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('thieve', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea6aaef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theft'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('theft', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a2f2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(tag): # get_pos() user defined function and tag user defined parameter\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "727cce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inbuilt library averaged_perceptron_tagger to be downloaded\n",
    "# nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d9bdd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian football is an undiscovered diamond mine\n"
     ]
    }
   ],
   "source": [
    "# for example\n",
    "sentence=\"Indian football is an undiscovered diamond mine\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "246b98c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Indian', 'football', 'is', 'an', 'undiscovered', 'diamond', 'mine']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first apply word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2641716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Indian', 'JJ'),\n",
       " ('football', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('undiscovered', 'JJ'),\n",
       " ('diamond', 'NN'),\n",
       " ('mine', 'NN')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tag=nltk.pos_tag(words) # word_tag is user defined\n",
    "word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3d73a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c64ba1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('Indian', 'JJ')\n",
      "1 ('football', 'NN')\n",
      "2 ('is', 'VBZ')\n",
      "3 ('an', 'DT')\n",
      "4 ('undiscovered', 'JJ')\n",
      "5 ('diamond', 'NN')\n",
      "6 ('mine', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(word_tag):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83ffbf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian football be an undiscovered diamond mine "
     ]
    }
   ],
   "source": [
    "for word, tag in word_tag:\n",
    "    lemma1=lemma.lemmatize(word, pos=get_pos(tag))\n",
    "    print(lemma1, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d541a097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study', 'studies', 'studying', 'studied', 'history', 'historical', 'went', 'go', 'going']\n"
     ]
    }
   ],
   "source": [
    "# 2nd example for stemming\n",
    "# Create a list\n",
    "words=['study', 'studies', 'studying', 'studied', 'history', 'historical', 'went', 'go', 'going']\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5435da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stemming process\n",
    "# Create object of PorterStemmer class\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76c5fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'studi', 'studi', 'histori', 'histor', 'went', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "# To declare empty list\n",
    "result=[]\n",
    "for w in words:\n",
    "    result.append(porter.stem(w))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f5263c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'studi', 'studi', 'studi', 'histori', 'histor', 'went', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "# list comprehension\n",
    "result=[porter.stem(w) for w in words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1b2ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study', 'study', 'studying', 'studied', 'history', 'historical', 'went', 'go', 'going']\n",
      "['study', 'study', 'study', 'study', 'history', 'historical', 'go', 'go', 'go']\n"
     ]
    }
   ],
   "source": [
    "# Applying lemmatization on words list\n",
    "# list comprehension\n",
    "result=[lemma.lemmatize(w) for w in words]\n",
    "print(result)\n",
    "\n",
    "result=[lemma.lemmatize(w, pos=wordnet.VERB) for w in words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef97605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVectorization is a simple technique of converting the text or paragraph into number format/numeric format. Because ML/DL \\nonly understands numerical data. So we convert text type data into vector format means numerical format, then we feed data\\nto machine or machine learning algorithm ot neural network.\\nThere are 2 types of Vectorization:\\n1. Count Vectorization\\n2. TF-IDF(Term Frequency-Inverse Document Frequency)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next step of preprocessing Vectorization:- \n",
    "'''\n",
    "Vectorization is a simple technique of converting the text or paragraph into number format/numeric format. Because ML/DL \n",
    "only understands numerical data. So we convert text type data into vector format means numerical format, then we feed data\n",
    "to machine or machine learning algorithm ot neural network.\n",
    "There are 2 types of Vectorization:\n",
    "1. Count Vectorization\n",
    "2. TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44c07beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorization/Beg of word(BOW)\n",
    "# for example: Suppose we have 4 datapoints means 4 records means we have complain data of customer\n",
    "\n",
    "d1=\"I am facing network issue please solve the network problem.\"\n",
    "d2=\"I am unable to make phone calls.\"\n",
    "d3=\"I am shifting kindly port my number.\"\n",
    "d4=\"I want to port my number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a9ec7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I am facing network issue please solve the net...\n",
       "1                     I am unable to make phone calls.\n",
       "2                 I am shifting kindly port my number.\n",
       "3                            I want to port my number.\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First collecting all the documents in the series and storing the variable called corpus.\n",
    "import pandas as pd\n",
    "corpus=pd.Series([d1, d2, d3, d4])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c839353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step: Now we have to convert corpus into vector means Number format using countvectorizer\n",
    "# call inbuilt class CountVectorizer inner class which defined in feature_extraction.text\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7edb33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object of CountVectorizer class\n",
    "cv=CountVectorizer(stop_words=\"english\")\n",
    "# if you want to remove stopwords while doing the vectorization, so pass the parameter stop_words=\"english\"\n",
    "# but it is totally user's choice to do it. Otherwise you have to remove all the parameters before doing this process.\n",
    "\n",
    "# cv=CountVectorizer() this command won't remove the stopwords and punctuation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25662401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting given documents which hold in corpus is converted into vector, use inbuilt method fit_transform() of CountVectorizer class\n",
    "df=cv.fit_transform(corpus)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "044fe2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['calls', 'facing', 'issue', 'kindly', 'make', 'network', 'number',\n",
       "       'phone', 'port', 'problem', 'shifting', 'solve', 'unable', 'want'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show all columns, use inbuilt method get_feature_names() of CountVectorizer class\n",
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf94a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show rows, use inbuilt method toarray()\n",
    "df.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15092414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>facing</th>\n",
       "      <th>issue</th>\n",
       "      <th>kindly</th>\n",
       "      <th>make</th>\n",
       "      <th>network</th>\n",
       "      <th>number</th>\n",
       "      <th>phone</th>\n",
       "      <th>port</th>\n",
       "      <th>problem</th>\n",
       "      <th>shifting</th>\n",
       "      <th>solve</th>\n",
       "      <th>unable</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  facing  issue  kindly  make  network  number  phone  port  problem  \\\n",
       "0      0       1      1       0     0        2       0      0     0        1   \n",
       "1      1       0      0       0     1        0       0      1     0        0   \n",
       "2      0       0      0       1     0        0       1      0     1        0   \n",
       "3      0       0      0       0     0        0       1      0     1        0   \n",
       "\n",
       "   shifting  solve  unable  want  \n",
       "0         0      1       0     0  \n",
       "1         0      0       1     0  \n",
       "2         1      0       0     0  \n",
       "3         0      0       0     1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe, which consists of rows and columns\n",
    "df=pd.DataFrame(df.toarray(), columns=cv.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b77236b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFormula:\\nTF(t, d)= number of times t appears in d/Total number of terms in d\\nwhere t is words and d is the sentence.\\n\\nIDF=log(No. of sentences/No. of sentences containing words)\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "'''\n",
    "Formula:\n",
    "TF(t, d)= number of times t appears in d/Total number of terms in d\n",
    "where t is words and d is the sentence.\n",
    "\n",
    "IDF=log(No. of sentences/No. of sentences containing words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2d35d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking same above example\n",
    "d1=\"I am facing network issue please solve the network problem.\"\n",
    "d2=\"I am unable to make phone calls.\"\n",
    "d3=\"I am shifting kindly port my number.\"\n",
    "d4=\"I want to port my number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cee89871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step: Now we have to convert corpus into vector means Number format \n",
    "# call inbuilt class TfidfVectorizer inner class which defined in feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b99a776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object of CountVectorizer class\n",
    "tfidf=TfidfVectorizer(stop_words=\"english\")\n",
    "# if you want to remove stopwords while doing the vectorization, so pass the parameter stop_words=\"english\"\n",
    "# but it is totally user's choice to do it. Otherwise you have to remove all the parameters before doing this process.\n",
    "\n",
    "# cv=TfidfVectorizer() this command won't remove the stopwords and punctuation automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7c0f167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting given documents which hold in corpus is converted into vector, use inbuilt method fit_transform() of TfidfVectorizer class\n",
    "df=tfidf.fit_transform(corpus)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "300fa92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>facing</th>\n",
       "      <th>issue</th>\n",
       "      <th>kindly</th>\n",
       "      <th>make</th>\n",
       "      <th>network</th>\n",
       "      <th>number</th>\n",
       "      <th>phone</th>\n",
       "      <th>port</th>\n",
       "      <th>problem</th>\n",
       "      <th>shifting</th>\n",
       "      <th>solve</th>\n",
       "      <th>unable</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls    facing     issue    kindly  make   network    number  phone  \\\n",
       "0    0.0  0.353553  0.353553  0.000000   0.0  0.707107  0.000000    0.0   \n",
       "1    0.5  0.000000  0.000000  0.000000   0.5  0.000000  0.000000    0.5   \n",
       "2    0.0  0.000000  0.000000  0.555283   0.0  0.000000  0.437791    0.0   \n",
       "3    0.0  0.000000  0.000000  0.000000   0.0  0.000000  0.526405    0.0   \n",
       "\n",
       "       port   problem  shifting     solve  unable      want  \n",
       "0  0.000000  0.353553  0.000000  0.353553     0.0  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000     0.5  0.000000  \n",
       "2  0.437791  0.000000  0.555283  0.000000     0.0  0.000000  \n",
       "3  0.526405  0.000000  0.000000  0.000000     0.0  0.667679  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe, which consists of rows and columns\n",
    "df=pd.DataFrame(df.toarray(), columns=tfidf.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b435742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
